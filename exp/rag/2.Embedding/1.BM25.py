#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BM25ç®—æ³•æ•™å­¦æ¡ˆä¾‹ - æ‹çˆ±æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿ
========================================

è¿™æ˜¯ä¸€ä¸ªç”ŸåŠ¨å½¢è±¡çš„BM25ç®—æ³•æ•™å­¦æ¡ˆä¾‹ï¼Œé€šè¿‡æ‹çˆ±ç›¸å…³çš„æ–‡æ¡£æ¥æ¼”ç¤º
åœ¨RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰åœºæ™¯ä¸­å¦‚ä½•ä½¿ç”¨BM25è¿›è¡Œæ–‡æ¡£æ£€ç´¢ã€‚

BM25ï¼ˆBest Matching 25ï¼‰æ˜¯ä¸€ç§ç»å…¸çš„ä¿¡æ¯æ£€ç´¢ç®—æ³•ï¼Œ
å¹¿æ³›åº”ç”¨äºæœç´¢å¼•æ“å’Œæ–‡æ¡£æ£€ç´¢ç³»ç»Ÿä¸­ã€‚
"""

import math
import re
from collections import Counter
from typing import List, Dict, Tuple

import jieba  # ä¸­æ–‡åˆ†è¯åº“


class BM25TeachingDemo:
    """
    BM25ç®—æ³•æ•™å­¦æ¼”ç¤ºç±»
    
    è¿™ä¸ªç±»å°†é€šè¿‡æ‹çˆ±æ–‡æ¡£æ£€ç´¢çš„ä¾‹å­ï¼Œç”ŸåŠ¨åœ°å±•ç¤ºBM25ç®—æ³•çš„å·¥ä½œåŸç†
    """

    def __init__(self, k1: float = 1.5, b: float = 0.75):
        """
        åˆå§‹åŒ–BM25å‚æ•°
        
        Args:
            k1: æ§åˆ¶è¯é¢‘é¥±å’Œåº¦çš„å‚æ•°ï¼Œé€šå¸¸å–å€¼1.2-2.0
            b: æ§åˆ¶æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–çš„å‚æ•°ï¼Œé€šå¸¸å–å€¼0.75
        """
        self.k1 = k1
        self.b = b
        self.documents = []  # å­˜å‚¨åŸå§‹æ–‡æ¡£
        self.tokenized_docs = []  # å­˜å‚¨åˆ†è¯åçš„æ–‡æ¡£
        self.doc_freqs = []  # å­˜å‚¨æ¯ä¸ªæ–‡æ¡£çš„è¯é¢‘
        self.idf_values = {}  # å­˜å‚¨æ¯ä¸ªè¯çš„IDFå€¼
        self.avg_doc_length = 0  # å¹³å‡æ–‡æ¡£é•¿åº¦

        print("ğŸŒ¹ æ¬¢è¿æ¥åˆ°BM25æ‹çˆ±æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿï¼ ğŸŒ¹")
        print("æˆ‘ä»¬å°†é€šè¿‡æ‹çˆ±ç›¸å…³çš„æ–‡æ¡£æ¥å­¦ä¹ BM25ç®—æ³•~")

    def prepare_love_documents(self):
        """
        å‡†å¤‡æ‹çˆ±ç›¸å…³çš„ç¤ºä¾‹æ–‡æ¡£
        è¿™äº›æ–‡æ¡£å°†ä½œä¸ºæˆ‘ä»¬çš„çŸ¥è¯†åº“
        """
        self.documents = [
            {
                "id": 1,
                "title": "åˆæ¬¡çº¦ä¼šæ”»ç•¥",
                "content": "åˆæ¬¡çº¦ä¼šæ˜¯æ‹çˆ±å…³ç³»ä¸­éå¸¸é‡è¦çš„ä¸€æ­¥ã€‚é€‰æ‹©åˆé€‚çš„åœ°ç‚¹å¾ˆå…³é”®ï¼Œå¯ä»¥é€‰æ‹©å’–å•¡å…ã€ç”µå½±é™¢æˆ–è€…å…¬å›­ã€‚è¦æ³¨æ„ç©¿ç€å¾—ä½“ï¼Œä¿æŒè‰¯å¥½çš„è°ˆè¯æ°›å›´ã€‚è®°ä½è¦å‡†æ—¶åˆ°è¾¾ï¼Œå±•ç°ä½ çš„è¯šæ„å’Œå°Šé‡ã€‚"
            },
            {
                "id": 2,
                "title": "å¦‚ä½•è¡¨ç™½æˆåŠŸ",
                "content": "è¡¨ç™½æ˜¯æ‹çˆ±ä¸­çš„å…³é”®æ—¶åˆ»ã€‚é¦–å…ˆè¦é€‰æ‹©åˆé€‚çš„æ—¶æœºå’Œåœ°ç‚¹ï¼Œç¡®ä¿å¯¹æ–¹å¿ƒæƒ…æ„‰å¿«ã€‚è¡¨ç™½æ—¶è¦çœŸè¯šï¼Œè¯´å‡ºè‡ªå·±çš„çœŸå®æ„Ÿå—ã€‚ä¸è¦ç»™å¯¹æ–¹å¤ªå¤§å‹åŠ›ï¼Œè¦ç»™å¥¹æ€è€ƒçš„æ—¶é—´ã€‚è®°ä½ï¼ŒçœŸè¯šæ˜¯æœ€é‡è¦çš„ã€‚"
            },
            {
                "id": 3,
                "title": "æ‹çˆ±ä¸­çš„æ²Ÿé€šæŠ€å·§",
                "content": "è‰¯å¥½çš„æ²Ÿé€šæ˜¯æ‹çˆ±å…³ç³»çš„åŸºç¡€ã€‚è¦å­¦ä¼šå€¾å¬å¯¹æ–¹çš„æƒ³æ³•å’Œæ„Ÿå—ï¼Œä¸è¦æ€¥äºåé©³ã€‚è¡¨è¾¾è‡ªå·±è§‚ç‚¹æ—¶è¦æ¸©å’Œè€Œåšå®šã€‚é‡åˆ°åˆ†æ­§æ—¶ï¼Œè¦ä¿æŒå†·é™ï¼Œå¯»æ‰¾å…±åŒç‚¹ã€‚è®°ä½æ²Ÿé€šæ˜¯åŒå‘çš„ï¼Œè¦ç»™å¯¹æ–¹è¡¨è¾¾çš„æœºä¼šã€‚"
            },
            {
                "id": 4,
                "title": "ç»´æŒé•¿ä¹…å…³ç³»çš„ç§˜è¯€",
                "content": "é•¿ä¹…çš„æ‹çˆ±å…³ç³»éœ€è¦åŒæ–¹çš„åŠªåŠ›å’Œç»´æŠ¤ã€‚è¦ä¿æŒæ–°é²œæ„Ÿï¼Œç»å¸¸åˆ›é€ æƒŠå–œå’Œæµªæ¼«ã€‚å­¦ä¼šåŒ…å®¹å¯¹æ–¹çš„ç¼ºç‚¹ï¼ŒåŒæ—¶ä¹Ÿè¦æˆé•¿è‡ªå·±ã€‚å»ºç«‹å…±åŒçš„ç›®æ ‡å’Œå…´è¶£çˆ±å¥½ã€‚æœ€é‡è¦çš„æ˜¯è¦ä¿¡ä»»å¯¹æ–¹ï¼Œç»™å½¼æ­¤è¶³å¤Ÿçš„ç©ºé—´ã€‚"
            },
            {
                "id": 5,
                "title": "åˆ†æ‰‹åçš„å¿ƒç†è°ƒé€‚",
                "content": "åˆ†æ‰‹æ˜¯æ‹çˆ±ä¸­ç—›è‹¦ä½†æœ‰æ—¶ä¸å¯é¿å…çš„ç»å†ã€‚è¦æ¥å—è‡ªå·±çš„æƒ…ç»ªï¼Œå…è®¸è‡ªå·±æ‚²ä¼¤ä¸€æ®µæ—¶é—´ã€‚ä¸è¦æ€¥äºå¼€å§‹æ–°çš„æ‹æƒ…ï¼Œå…ˆå¥½å¥½ç…§é¡¾è‡ªå·±ã€‚å¯ä»¥é€šè¿‡è¿åŠ¨ã€æ—…è¡Œæˆ–å­¦ä¹ æ–°æŠ€èƒ½æ¥è½¬ç§»æ³¨æ„åŠ›ã€‚è®°ä½æ¯ä¸€æ¬¡ç»å†éƒ½æ˜¯æˆé•¿çš„æœºä¼šã€‚"
            }
        ]

        print(f"\nğŸ“š å·²å‡†å¤‡{len(self.documents)}ç¯‡æ‹çˆ±ç›¸å…³æ–‡æ¡£ï¼š")
        for doc in self.documents:
            print(f"  ğŸ“„ {doc['title']}")

    def tokenize_text(self, text: str) -> List[str]:
        """
        ä¸­æ–‡æ–‡æœ¬åˆ†è¯
        
        Args:
            text: è¾“å…¥çš„ä¸­æ–‡æ–‡æœ¬
            
        Returns:
            åˆ†è¯åçš„è¯è¯­åˆ—è¡¨
        """
        # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
        words = jieba.cut(text, cut_all=False)

        # è¿‡æ»¤æ‰æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼ï¼Œåªä¿ç•™æœ‰æ„ä¹‰çš„è¯
        filtered_words = []
        for word in words:
            word = word.strip()
            if len(word) > 1 and not re.match(r'^[^\w]+$', word):
                filtered_words.append(word.lower())

        return filtered_words

    def build_index(self):
        """
        æ„å»ºBM25ç´¢å¼•
        è¿™ä¸€æ­¥ä¼šå¯¹æ‰€æœ‰æ–‡æ¡£è¿›è¡Œåˆ†è¯ï¼Œå¹¶è®¡ç®—å¿…è¦çš„ç»Ÿè®¡ä¿¡æ¯
        """
        print("\nğŸ”§ å¼€å§‹æ„å»ºBM25ç´¢å¼•...")

        # 1. å¯¹æ‰€æœ‰æ–‡æ¡£è¿›è¡Œåˆ†è¯
        for doc in self.documents:
            full_text = doc['title'] + ' ' + doc['content']
            tokens = self.tokenize_text(full_text)
            self.tokenized_docs.append(tokens)

            # è®¡ç®—è¯é¢‘
            word_freq = Counter(tokens)
            self.doc_freqs.append(word_freq)

        # 2. è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦
        total_length = sum(len(doc) for doc in self.tokenized_docs)
        self.avg_doc_length = total_length / len(self.tokenized_docs)

        # 3. è®¡ç®—IDFå€¼ï¼ˆé€†æ–‡æ¡£é¢‘ç‡ï¼‰
        all_words = set()
        for tokens in self.tokenized_docs:
            all_words.update(tokens)

        for word in all_words:
            # è®¡ç®—åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°é‡
            doc_count = sum(1 for doc_freq in self.doc_freqs if word in doc_freq)

            # è®¡ç®—IDFï¼šlog((N - df + 0.5) / (df + 0.5))
            # Næ˜¯æ€»æ–‡æ¡£æ•°ï¼Œdfæ˜¯åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°
            N = len(self.documents)
            idf = math.log((N - doc_count + 0.5) / (doc_count + 0.5))
            self.idf_values[word] = idf

        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")
        print(f"   ğŸ“Š æ€»æ–‡æ¡£æ•°: {len(self.documents)}")
        print(f"   ğŸ“Š å¹³å‡æ–‡æ¡£é•¿åº¦: {self.avg_doc_length:.2f} è¯")
        print(f"   ğŸ“Š è¯æ±‡è¡¨å¤§å°: {len(self.idf_values)} ä¸ªè¯")

    def calculate_bm25_score(self, query_tokens: List[str], doc_index: int) -> float:
        """
        è®¡ç®—æŸ¥è¯¢ä¸ç‰¹å®šæ–‡æ¡£çš„BM25å¾—åˆ†
        
        Args:
            query_tokens: æŸ¥è¯¢çš„åˆ†è¯ç»“æœ
            doc_index: æ–‡æ¡£ç´¢å¼•
            
        Returns:
            BM25å¾—åˆ†
        """
        score = 0.0
        doc_freq = self.doc_freqs[doc_index]
        doc_length = len(self.tokenized_docs[doc_index])

        for word in query_tokens:
            if word in doc_freq:
                # è·å–è¯é¢‘å’ŒIDF
                tf = doc_freq[word]  # è¯é¢‘
                idf = self.idf_values.get(word, 0)  # IDFå€¼

                # BM25å…¬å¼çš„æ ¸å¿ƒè®¡ç®—
                # score += IDF * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length / avg_doc_length)))
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avg_doc_length))

                word_score = idf * (numerator / denominator)
                score += word_score

                # è¯¦ç»†å±•ç¤ºè®¡ç®—è¿‡ç¨‹ï¼ˆç”¨äºæ•™å­¦ï¼‰
                print(f"    ğŸ” è¯è¯­ '{word}': tf={tf}, idf={idf:.3f}, è´¡çŒ®å¾—åˆ†={word_score:.3f}")

        return score

    def search(self, query: str, top_k: int = 3) -> List[Tuple[Dict, float]]:
        """
        ä½¿ç”¨BM25ç®—æ³•æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›å‰kä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
            
        Returns:
            æ’åºåçš„æ–‡æ¡£å’Œå¾—åˆ†åˆ—è¡¨
        """
        print(f"\nğŸ” ç”¨æˆ·æŸ¥è¯¢: '{query}'")
        print("=" * 50)

        # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
        query_tokens = self.tokenize_text(query)
        print(f"ğŸ“ æŸ¥è¯¢åˆ†è¯ç»“æœ: {query_tokens}")

        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„BM25å¾—åˆ†
        results = []
        for i, doc in enumerate(self.documents):
            print(f"\nğŸ“„ è®¡ç®—æ–‡æ¡£ '{doc['title']}' çš„ç›¸å…³åº¦:")
            score = self.calculate_bm25_score(query_tokens, i)
            results.append((doc, score))
            print(f"   ğŸ¯ æ€»å¾—åˆ†: {score:.3f}")

        # æŒ‰å¾—åˆ†æ’åº
        results.sort(key=lambda x: x[1], reverse=True)

        print(f"\nğŸ† æ£€ç´¢ç»“æœ (Top {top_k}):")
        print("=" * 50)

        top_results = results[:top_k]
        for i, (doc, score) in enumerate(top_results, 1):
            print(f"{i}. ğŸ“š {doc['title']}")
            print(f"   ğŸ¯ ç›¸å…³åº¦å¾—åˆ†: {score:.3f}")
            print(f"   ğŸ“– å†…å®¹é¢„è§ˆ: {doc['content'][:50]}...")
            print()

        return top_results

    def explain_bm25_formula(self):
        """
        è¯¦ç»†è§£é‡ŠBM25å…¬å¼çš„å«ä¹‰
        """
        print("\nğŸ“– BM25å…¬å¼è¯¦è§£:")
        print("=" * 50)
        print("BM25(q,d) = Î£ IDF(w) * (tf(w,d) * (k1 + 1)) / (tf(w,d) + k1 * (1 - b + b * |d|/avgdl))")
        print()
        print("ğŸ”¤ ç¬¦å·è¯´æ˜:")
        print("  â€¢ q: æŸ¥è¯¢ (query)")
        print("  â€¢ d: æ–‡æ¡£ (document)")
        print("  â€¢ w: æŸ¥è¯¢ä¸­çš„è¯è¯­ (word)")
        print("  â€¢ tf(w,d): è¯è¯­wåœ¨æ–‡æ¡£dä¸­çš„é¢‘ç‡")
        print("  â€¢ IDF(w): è¯è¯­wçš„é€†æ–‡æ¡£é¢‘ç‡")
        print("  â€¢ |d|: æ–‡æ¡£dçš„é•¿åº¦")
        print("  â€¢ avgdl: å¹³å‡æ–‡æ¡£é•¿åº¦")
        print("  â€¢ k1, b: è°ƒèŠ‚å‚æ•°")
        print()
        print("ğŸ¯ å‚æ•°ä½œç”¨:")
        print(f"  â€¢ k1 = {self.k1}: æ§åˆ¶è¯é¢‘é¥±å’Œåº¦ï¼Œé¿å…é«˜é¢‘è¯è¿‡åº¦å½±å“")
        print(f"  â€¢ b = {self.b}: æ§åˆ¶æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–ï¼Œå¹³è¡¡é•¿çŸ­æ–‡æ¡£")
        print()
        print("ğŸ’¡ ç®—æ³•æ€æƒ³:")
        print("  1. è¯é¢‘è¶Šé«˜ï¼Œç›¸å…³åº¦è¶Šé«˜ï¼ˆä½†æœ‰é¥±å’Œç‚¹ï¼‰")
        print("  2. åŒ…å«è¯¥è¯çš„æ–‡æ¡£è¶Šå°‘ï¼Œè¯¥è¯è¶Šé‡è¦ï¼ˆIDFï¼‰")
        print("  3. è€ƒè™‘æ–‡æ¡£é•¿åº¦ï¼Œé¿å…é•¿æ–‡æ¡£å ä¼˜åŠ¿")


def demonstrate_bm25_love_search():
    """
    æ¼”ç¤ºBM25æ‹çˆ±æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿ
    """
    print("ğŸŒ¹ğŸ’• BM25æ‹çˆ±æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿæ¼”ç¤º ğŸ’•ğŸŒ¹")
    print("=" * 60)

    # åˆ›å»ºBM25æ£€ç´¢ç³»ç»Ÿ
    bm25_demo = BM25TeachingDemo()

    # å‡†å¤‡æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•
    bm25_demo.prepare_love_documents()
    bm25_demo.build_index()

    # è§£é‡ŠBM25å…¬å¼
    bm25_demo.explain_bm25_formula()

    # æ¼”ç¤ºä¸åŒçš„æŸ¥è¯¢åœºæ™¯
    test_queries = [
        "å¦‚ä½•è¿›è¡Œç¬¬ä¸€æ¬¡çº¦ä¼š",
        "è¡¨ç™½çš„æŠ€å·§å’Œæ–¹æ³•",
        "æ‹çˆ±ä¸­æ€ä¹ˆæ²Ÿé€š",
        "åˆ†æ‰‹äº†å¾ˆéš¾è¿‡æ€ä¹ˆåŠ"
    ]

    for query in test_queries:
        bm25_demo.search(query, top_k=3)
        input("\næŒ‰å›è½¦é”®ç»§ç»­ä¸‹ä¸€ä¸ªæŸ¥è¯¢æ¼”ç¤º...")

    print("\nğŸ“ æ•™å­¦æ€»ç»“:")
    print("=" * 50)
    print("1. BM25æ˜¯ç»å…¸çš„ä¿¡æ¯æ£€ç´¢ç®—æ³•ï¼Œåœ¨RAGç³»ç»Ÿä¸­å¹¿æ³›åº”ç”¨")
    print("2. é€šè¿‡è¯é¢‘ã€é€†æ–‡æ¡£é¢‘ç‡å’Œæ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–æ¥è®¡ç®—ç›¸å…³åº¦")
    print("3. ç›¸æ¯”ç®€å•çš„è¯é¢‘åŒ¹é…ï¼ŒBM25èƒ½æ›´å¥½åœ°å¤„ç†é•¿æ–‡æ¡£å’Œå¸¸è§è¯")
    print("4. åœ¨å®é™…RAGåº”ç”¨ä¸­ï¼Œé€šå¸¸ä¸å‘é‡æ£€ç´¢ç»“åˆä½¿ç”¨ï¼Œæé«˜å¬å›æ•ˆæœ")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    demonstrate_bm25_love_search()
